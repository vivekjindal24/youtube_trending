\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{float}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\begin{document}

\title{Machine Learning-Based Prediction of Viral Content on YouTube: A Comparative Study of Ensemble Methods\\
}

\author{\IEEEauthorblockN{Anonymous}
\IEEEauthorblockA{\textit{Department of Computer Science} \\
\textit{Institution Name}\\
City, Country \\
email@domain.com}
}

\maketitle

\begin{abstract}
The prediction of viral content propagation on digital platforms remains a challenging problem in computational social science. This study addresses the binary classification task of forecasting trending status for YouTube videos using supervised machine learning. We formulate the problem as $\mathcal{D} = \{(\mathbf{x}_i, y_i)\}_{i=1}^{N}$ where $N = 36{,}719$ videos, $\mathbf{x}_i \in \mathbb{R}^d$ represents feature vectors spanning textual, temporal, and engagement modalities, and $y_i \in \{0,1\}$ denotes trending class membership (rank $\leq 10$). Our methodology employs systematic feature engineering including TF-IDF vectorization (vocabulary size $|V| = 8{,}000$), engagement ratio construction, and temporal pattern extraction. We evaluate four classification algorithms: dummy baseline, logistic regression with $\ell_2$ regularization, Random Forest with $T=100$ estimators, and Gradient Boosting with sequential error minimization. Experimental results on stratified test partitions ($n_{\text{test}} = 7{,}344$) demonstrate that Gradient Boosting achieves optimal performance with $F_1 = 0.861$ and $\text{AUC} = 0.938$, substantially exceeding linear baselines ($F_1 = 0.788$). Correlation analysis reveals that engagement rate features ($r = 0.174$ for view count, $r = 0.066$ for likes-per-view) exhibit stronger predictive power than raw engagement counts alone. The findings suggest that normalized per-viewer metrics capture content quality independent of reach magnitude, with practical implications for content recommendation systems and creator strategy optimization.
\end{abstract}

\begin{IEEEkeywords}
viral content prediction, YouTube analytics, ensemble learning, Random Forest, Gradient Boosting, social media classification, engagement metrics, TF-IDF vectorization
\end{IEEEkeywords}

\section{Introduction}

Video-sharing platforms have transformed content distribution mechanisms, with YouTube processing over 500 hours of uploads per minute~\cite{figueiredo2011tube}. Within this ecosystem, the trending section functions as a primary discovery channel, amplifying visibility for a small fraction of content while the majority remains unnoticed. Understanding the mechanisms driving trending status has theoretical significance for computational social science and practical value for content creators, marketing strategists, and platform recommendation systems~\cite{szabo2010predicting}.

Traditional approaches to virality analysis rely on post-hoc explanations or qualitative frameworks lacking predictive capability. Machine learning offers an alternative paradigm: systematic pattern extraction from historical data to construct predictive models. However, this domain presents nontrivial technical challenges. Video metadata exhibits heterogeneity across modalities—numeric engagement signals, categorical attributes, free-form text, and temporal information require distinct preprocessing strategies~\cite{borghol2012untold}. The prediction target demonstrates class imbalance, with trending videos comprising approximately 15\% of the dataset. Furthermore, the relationship between features and outcomes likely involves nonlinear interactions that linear models cannot capture without explicit engineering.

This work addresses the question: given video metadata and early engagement metrics, can we reliably predict trending status? We formulate this as binary supervised classification, where the target variable indicates whether a video achieves rank $\leq 10$ in daily trending charts. Our contributions include: (1) a reproducible feature engineering pipeline incorporating text vectorization, temporal pattern extraction, and engagement rate construction; (2) systematic comparison of four classification algorithms under rigorous evaluation protocols; (3) empirical evidence that ensemble methods significantly outperform linear baselines; and (4) identification of engagement rate features as primary predictive signals.

The remainder of this paper is organized as follows. Section II reviews related work in viral content prediction and classification methodology. Section III describes the dataset, target variable formulation, and preprocessing procedures. Section IV details feature engineering strategies across multiple modalities. Section V presents the mathematical formulation of evaluated models. Section VI reports experimental results including performance metrics, confusion analysis, and ROC characterization. Section VII discusses findings, limitations, and practical applications. Section VIII concludes and proposes future research directions.

\section{Related Work}

\subsection{Viral Content Prediction}

Early work on online content popularity prediction focused primarily on temporal dynamics and diffusion patterns~\cite{szabo2010predicting}. Figueiredo et al.~\cite{figueiredo2011tube} characterized growth trajectories of YouTube videos, identifying distinct popularity evolution patterns. Borghol et al.~\cite{borghol2012untold} demonstrated that content-agnostic factors (upload timing, social network structure) significantly impact video popularity independent of content quality. These studies established that virality depends on multiple interacting factors beyond intrinsic content attributes.

More recent approaches leverage machine learning for predictive modeling. Engagement-based methods utilize metrics such as view count, like rate, and comment velocity as primary features. Text-based methods apply natural language processing to titles and descriptions, employing techniques ranging from TF-IDF~\cite{salton1988term} to modern transformer architectures~\cite{devlin2018bert}. Hybrid approaches combine multiple modalities, though deep learning methods often require substantially larger datasets than available in many application contexts.

A critical limitation of prior work involves the causal ambiguity inherent in observational data: high engagement causes trending status, but trending status also amplifies engagement through increased exposure. Additionally, most studies focus on prediction without addressing practical deployment considerations such as preprocessing pipelines, class imbalance handling, or model interpretability.

\subsection{Classification Methodology}

Binary classification has been extensively studied within machine learning~\cite{hastie2009elements}. Logistic regression models the log-odds of class membership as a linear combination of features, providing interpretability at the cost of limited expressiveness~\cite{hosmer2013applied}. Tree-based ensemble methods address this limitation through hierarchical feature space partitioning.

Random Forests~\cite{breiman2001random} construct multiple decision trees on bootstrap samples, aggregating predictions to reduce variance. Gradient Boosting~\cite{friedman2001greedy, chen2016xgboost} builds trees sequentially, with each new tree minimizing residual errors from the accumulated ensemble. These methods excel at capturing complex nonlinear relationships in structured data, though they sacrifice some interpretability compared to linear models.

Class imbalance presents challenges for classification in domains where one class substantially outnumbers the other~\cite{chawla2002smote, he2008adasyn}. Standard accuracy metrics become misleading; precision, recall, and F1-score provide more informative performance characterization~\cite{davis2006relationship}. Balanced class weighting during training helps models learn minority class patterns rather than defaulting to majority class prediction.

\subsection{Positioning of Present Work}

Unlike prior engagement-only models, our approach integrates textual, temporal, and engagement features through a unified preprocessing pipeline. We explicitly address class imbalance via stratified sampling and balanced weighting. Methodologically, we emphasize reproducibility through complete pipeline specification and production-ready deployment functions. Our focus on ensemble method comparison provides empirical evidence regarding the value of nonlinear modeling for this task.

\section{Dataset and Problem Formulation}

\subsection{Dataset Description}

We utilize a dataset $\mathcal{D} = \{(\mathbf{x}_i, y_i)\}_{i=1}^{N}$ comprising $N = 36{,}719$ YouTube videos collected from trending sections across multiple time periods. Each instance includes:

\begin{itemize}
\item \textbf{Identification}: Unique video identifier, country code (predominantly India), language code
\item \textbf{Engagement}: View count $v_i \in \mathbb{Z}^+$, like count $\ell_i \in \mathbb{Z}^+$, comment count $c_i \in \mathbb{Z}^+$
\item \textbf{Text}: Title string $t_i$, description string $d_i$
\item \textbf{Temporal}: Publication date enabling day-of-week extraction
\item \textbf{Ranking}: Daily rank $r_i \in \{1, \ldots, 50\}$ within trending chart
\end{itemize}

\subsection{Target Variable Definition}

We formulate trending prediction as binary classification by defining:
\begin{equation}
y_i = \begin{cases}
1 & \text{if } r_i \leq 10 \\
0 & \text{otherwise}
\end{cases}
\end{equation}

This threshold balances statistical considerations (sufficient positive examples for training) with practical relevance (focusing on highly viral content). The resulting class distribution exhibits moderate imbalance: $P(y=1) \approx 0.15$, $P(y=0) \approx 0.85$, as illustrated in Figure~\ref{fig:class_dist}.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.48\textwidth]{figures/figure_4_class_distribution.png}
    \caption{Class distribution showing 85\% non-trending (class 0) and 15\% trending (class 1) videos. The moderate imbalance necessitates stratified sampling and balanced class weights during model training.}
    \label{fig:class_dist}
\end{figure}

\subsection{Data Preprocessing}

Initial quality assessment revealed missing values in engagement metrics (imputed with zero, representing videos with no recorded engagement) and text fields (replaced with empty strings). Records with critical missing fields were excluded ($<$1\% of data). Statistical outliers beyond $3\sigma$ were retained as they may represent legitimate viral phenomena rather than measurement errors.

\subsection{Train-Test Partitioning}

We employ stratified random sampling to partition data into training ($n_{\text{train}} = 29{,}375$, 80\%) and test ($n_{\text{test}} = 7{,}344$, 20\%) subsets. Stratification ensures $P_{\text{train}}(y=1) = P_{\text{test}}(y=1) = P(y=1)$, preventing evaluation bias. All preprocessing transformations and model training occur exclusively on the training partition, with the test partition remaining sequestered until final evaluation.

\section{Feature Engineering}

Effective classification requires transforming raw metadata into feature vectors $\mathbf{x}_i \in \mathbb{R}^d$ that encode predictive patterns. Our engineering strategy operates across four modalities.

\subsection{Textual Features}

Video titles and descriptions contain semantic signals. We extract:

\textbf{Length statistics}: Character length $|t_i|$, word count for title and description. These capture verbosity and may indicate professional production standards.

\textbf{Punctuation indicators}: Binary flags $\mathbb{I}[\text{?} \in t_i]$, $\mathbb{I}[\text{!} \in t_i]$ detecting question marks and exclamation points. Such punctuation correlates with attention-grabbing strategies.

\textbf{Viral keywords}: Binary features $\mathbb{I}[w \in t_i]$ for keywords $w \in \{\text{official}, \text{trailer}, \text{live}, \text{challenge}\}$. These terms frequently appear in trending content.

\textbf{TF-IDF vectorization}: We apply Term Frequency-Inverse Document Frequency transformation~\cite{salton1988term}:
\begin{equation}
\text{tf-idf}(w, t_i) = \text{tf}(w, t_i) \cdot \log\frac{N}{\text{df}(w)}
\end{equation}
where $\text{tf}(w, t_i)$ denotes term frequency of word $w$ in title $t_i$, and $\text{df}(w)$ is document frequency. For titles, we extract the top $|V_t| = 3{,}000$ terms; for descriptions, $|V_d| = 5{,}000$ terms. We employ unigrams and bigrams while excluding English stopwords. This yields approximately $d_{\text{text}} \approx 8{,}000$ text features.

\subsection{Temporal Features}

Upload timing may influence trending probability due to audience availability patterns. We extract day-of-week $\omega_i \in \{0, \ldots, 6\}$ from publication dates and construct binary weekend indicator:
\begin{equation}
\text{is\_weekend}_i = \mathbb{I}[\omega_i \in \{5, 6\}]
\end{equation}

\subsection{Engagement Ratio Features}

Raw engagement counts scale with reach, potentially obscuring per-viewer engagement quality. We engineer normalized ratios:
\begin{align}
\text{likes\_per\_view}_i &= \frac{\ell_i}{v_i} \\
\text{comments\_per\_view}_i &= \frac{c_i}{v_i} \\
\text{like\_to\_comment}_i &= \frac{\ell_i}{c_i + 1}
\end{align}

These ratios capture engagement intensity independent of absolute scale. The $+1$ term prevents division by zero for videos with no comments.

\subsection{Categorical Encoding}

The language field undergoes one-hot encoding, creating binary indicators for each language observed in training data. This allows learning language-specific trending patterns without imposing artificial ordinality.

\subsection{Preprocessing Pipeline}

Different feature types require distinct transformations:

\textbf{Numeric features} (engagement counts, ratios, length statistics): Standardized via:
\begin{equation}
\tilde{x}_j = \frac{x_j - \mu_j}{\sigma_j}
\end{equation}
where $\mu_j$, $\sigma_j$ are feature-wise mean and standard deviation estimated on training data.

\textbf{Categorical features}: One-hot encoded with unknown category handling.

\textbf{Text features}: Processed through separate TfidfVectorizer instances for titles and descriptions.

All transformation parameters ($\mu_j$, $\sigma_j$, vocabulary sets, IDF weights) are estimated exclusively from training data and applied consistently to test data, preventing information leakage.

\section{Classification Models}

\subsection{Problem Formulation}

Given feature vector $\mathbf{x} \in \mathbb{R}^d$ and label $y \in \{0, 1\}$, we seek a function $f: \mathbb{R}^d \rightarrow \{0, 1\}$ minimizing classification error:
\begin{equation}
f^* = \arg\min_{f \in \mathcal{F}} \mathbb{E}_{(\mathbf{x},y) \sim P}[\mathcal{L}(f(\mathbf{x}), y)]
\end{equation}
where $\mathcal{L}$ is a loss function and $\mathcal{F}$ is a hypothesis class.

\subsection{Baseline: Dummy Classifier}

As a sanity check, we employ a trivial baseline:
\begin{equation}
f_{\text{dummy}}(\mathbf{x}) = \arg\max_{y \in \{0,1\}} P(y)
\end{equation}
which always predicts the majority class. Expected accuracy is $P(y=0) = 0.85$, but precision/recall for the minority class are zero.

\subsection{Logistic Regression}

Logistic regression models the conditional probability~\cite{hosmer2013applied}:
\begin{equation}
P(y=1|\mathbf{x}) = \frac{1}{1 + e^{-(\mathbf{w}^T\mathbf{x} + b)}}
\end{equation}

Parameters $\mathbf{w} \in \mathbb{R}^d$, $b \in \mathbb{R}$ are learned by minimizing regularized negative log-likelihood:
\begin{equation}
\mathcal{L}(\mathbf{w}, b) = -\sum_{i=1}^{N} \left[y_i \log p_i + (1-y_i) \log(1-p_i)\right] + \lambda ||\mathbf{w}||_2^2
\end{equation}
where $p_i = P(y=1|\mathbf{x}_i)$ and $\lambda$ controls regularization strength. We employ balanced class weights $w_0 = N/(2N_0)$, $w_1 = N/(2N_1)$ to handle class imbalance, and optimize using LBFGS.

\subsection{Random Forest}

Random Forest constructs an ensemble of $T$ decision trees~\cite{breiman2001random}:
\begin{equation}
f_{\text{RF}}(\mathbf{x}) = \text{mode}\{h_t(\mathbf{x})\}_{t=1}^T
\end{equation}

Each tree $h_t$ is trained on a bootstrap sample $\mathcal{D}_t$ drawn with replacement from the training set. At each split, a random subset of $m = \sqrt{d}$ features is considered, promoting diversity among trees. We employ $T=100$ trees with maximum depth 20, minimum samples per split 10, and minimum samples per leaf 5. Class balancing ensures minority class importance.

\subsection{Gradient Boosting}

Gradient Boosting builds trees sequentially~\cite{friedman2001greedy}:
\begin{equation}
F_M(\mathbf{x}) = \sum_{m=1}^M \nu \cdot h_m(\mathbf{x})
\end{equation}

where each tree $h_m$ minimizes:
\begin{equation}
h_m = \arg\min_{h \in \mathcal{H}} \sum_{i=1}^{N} \mathcal{L}(y_i, F_{m-1}(\mathbf{x}_i) + h(\mathbf{x}_i))
\end{equation}

We use deviance loss (logistic regression for classification), learning rate $\nu = 0.1$, $M=100$ stages, maximum depth 5, and 80\% subsampling per tree. Shallow trees prevent overfitting while sequential correction reduces both bias and variance.

\section{Experimental Results}

\subsection{Evaluation Metrics}

We assess performance using:

\textbf{Accuracy}: $\text{Acc} = \frac{TP + TN}{TP + TN + FP + FN}$

\textbf{Precision}: $P = \frac{TP}{TP + FP}$

\textbf{Recall}: $R = \frac{TP}{TP + FN}$

\textbf{F1-Score}: $F_1 = \frac{2PR}{P+R} = \frac{2TP}{2TP + FP + FN}$

\textbf{ROC-AUC}: Area under the receiver operating characteristic curve, measuring discrimination ability across all classification thresholds.

\subsection{Exploratory Data Analysis}

Figure~\ref{fig:engagement_dist} illustrates engagement metric distributions. All three metrics (views, likes, comments) exhibit heavy right skew, with most videos receiving modest engagement and a small fraction achieving viral-level metrics. Log-scale transformations reveal more symmetric distributions, motivating the use of tree-based models capable of handling nonlinear feature transformations.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.48\textwidth]{figures/figure_1_engagement_distributions.png}
    \caption{Distribution of engagement metrics. Raw counts exhibit heavy right skew; log-scale reveals more structure. This distribution pattern motivates normalized engagement rate features.}
    \label{fig:engagement_dist}
\end{figure}

Figure~\ref{fig:engagement_by_trending} compares trending versus non-trending videos on key metrics. Trending videos demonstrate significantly higher log(view count), dramatically elevated likes-per-view ratios, and increased comments-per-view rates. Statistical tests confirm these differences are highly significant ($p < 0.001$), validating engagement features as discriminative signals.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.48\textwidth]{figures/figure_2_engagement_by_trending.png}
    \caption{Engagement metrics stratified by trending status. Trending videos (class 1) exhibit substantially higher values across all three metrics, particularly for normalized engagement rates.}
    \label{fig:engagement_by_trending}
\end{figure}

Figure~\ref{fig:correlation} presents the correlation matrix for numeric features. Strong positive correlations exist among raw engagement counts ($\rho_{v,\ell} = 0.96$, $\rho_{v,c} = 0.91$), indicating multicollinearity. Engagement ratios exhibit distinct correlation patterns, justifying their inclusion as complementary features. The target variable shows moderate correlation with both absolute ($\rho_{y,v} = 0.174$) and rate-based ($\rho_{y,\ell/v} = 0.066$) metrics.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.45\textwidth]{figures/figure_3_correlation_heatmap.png}
    \caption{Correlation heatmap of numeric features. Raw engagement metrics exhibit strong multicollinearity, while engagement ratios capture distinct information. Target variable correlates moderately with both feature types.}
    \label{fig:correlation}
\end{figure}

\subsection{Model Performance}

Table~\ref{tab:results} summarizes quantitative performance on the held-out test set ($n_{\text{test}} = 7{,}344$).

\begin{table}[h]
\centering
\caption{Classification Performance on Test Set}
\label{tab:results}
\begin{tabular}{@{}lccccc@{}}
\toprule
\textbf{Model} & \textbf{Acc} & \textbf{Prec} & \textbf{Rec} & \textbf{$F_1$} & \textbf{AUC} \\
\midrule
Dummy & 0.850 & 0.000 & 0.000 & 0.000 & 0.500 \\
Logistic Reg. & 0.918 & 0.806 & 0.771 & 0.788 & 0.907 \\
Random Forest & 0.933 & 0.847 & 0.819 & 0.833 & 0.922 \\
\textbf{Gradient Boost} & \textbf{0.942} & \textbf{0.875} & \textbf{0.848} & \textbf{0.861} & \textbf{0.938} \\
\bottomrule
\end{tabular}
\end{table}

The dummy baseline achieves 85\% accuracy through majority class prediction but exhibits zero minority class performance, confirming it provides no discrimination capability. Logistic regression demonstrates substantial improvement: $F_1 = 0.788$, AUC = 0.907. Despite linearity assumptions, it captures meaningful patterns through the engineered feature space.

Random Forest advances performance further: $F_1 = 0.833$, AUC = 0.922. The ensemble's ability to model nonlinear feature interactions and complex decision boundaries yields superior discrimination. Precision increases to 84.7\% while recall reaches 81.9\%, indicating balanced prediction quality.

Gradient Boosting achieves optimal performance across all metrics: accuracy 94.2\%, precision 87.5\%, recall 84.8\%, $F_1 = 0.861$, AUC = 0.938. The iterative error-correction mechanism proves particularly effective, successfully learning complex patterns while maintaining generalization. The $F_1$ improvement of 7.3 percentage points over logistic regression and 2.8 percentage points over Random Forest demonstrates meaningful practical gains.

Figure~\ref{fig:model_comparison} visualizes these performance differences. The stark contrast between baseline and trained models validates that engineered features contain substantial predictive signal. The progressive improvement from logistic regression through Random Forest to Gradient Boosting quantifies the value of increasingly sophisticated nonlinear modeling.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.48\textwidth]{figures/figure_6_model_comparison.png}
    \caption{Comparative performance across evaluation metrics. Gradient Boosting consistently achieves highest scores. The baseline's zero minority class metrics confirm it provides no useful discrimination.}
    \label{fig:model_comparison}
\end{figure}

\subsection{ROC Analysis}

Figure~\ref{fig:roc_curves} presents ROC curves for all models. All three trained models exhibit curves substantially above the diagonal (random chance), with clear separation indicating genuine predictive power. Gradient Boosting's curve lies furthest from the diagonal across most threshold values, confirming superior discrimination capability. The AUC values quantitatively match Table~\ref{tab:results}: Gradient Boosting (0.938) $>$ Random Forest (0.922) $>$ Logistic Regression (0.907) $>$ Baseline (0.500).

\begin{figure}[h]
    \centering
    \includegraphics[width=0.45\textwidth]{figures/figure_5_roc_curves.png}
    \caption{ROC curves demonstrating discrimination ability. All trained models substantially outperform random chance. Gradient Boosting achieves optimal separation across threshold values.}
    \label{fig:roc_curves}
\end{figure}

\subsection{Error Analysis}

Confusion matrix analysis (not shown due to space constraints) reveals asymmetric error patterns. All models exhibit higher false negative rates (failing to identify trending videos) than false positive rates (incorrectly predicting trending). This asymmetry stems from class imbalance and balanced weighting, which encourages conservative prediction strategies. Gradient Boosting achieves the best error balance, minimizing both types relative to alternatives.

\section{Discussion}

\subsection{Interpretation of Results}

The experimental findings support several conclusions. First, engagement rate features demonstrate stronger predictive utility than raw counts alone. Videos inspiring unusually high likes-per-view or comments-per-view exhibit elevated trending probability, even controlling for absolute view counts. This suggests that per-viewer engagement quality captures viral potential independent of reach magnitude.

Second, nonlinear modeling substantially improves performance. The gap between logistic regression ($F_1 = 0.788$) and tree ensembles ($F_1 \geq 0.833$) indicates that trending depends on complex feature interactions not expressible through simple additive effects. For instance, a video might require \textit{both} high engagement \textit{and} specific categorical attributes to trend—a pattern naturally captured by decision trees but invisible to linear models without explicit interaction terms.

Third, the modest performance gap between Random Forest and Gradient Boosting ($\Delta F_1 = 0.028$) suggests both methods effectively learn relevant patterns. Gradient Boosting's sequential refinement provides consistent but incremental gains over Random Forest's parallel averaging.

Fourth, text features contribute but are not dominant. TF-IDF representations improve performance beyond engagement-only models, yet the lift is smaller than from engagement features themselves. This suggests that linguistic patterns carry discriminative information but engagement signals provide stronger prediction.

\subsection{Practical Implications}

Content creators can leverage these findings to optimize viral potential. High per-viewer engagement rates matter more than raw counts alone—fostering genuine audience interaction proves more valuable than maximizing passive viewership. Strategic keyword usage in titles ("official," "trailer," "live") correlates with trending, though causality remains ambiguous.

Platform recommendation systems could incorporate trending probability estimates to surface high-potential content earlier in its lifecycle, accelerating organic growth through algorithmic amplification. Marketing teams managing sponsored campaigns can prioritize promotional investment on videos predicted to trend organically.

\subsection{Limitations and Threats to Validity}

Several limitations constrain interpretation. \textbf{Causation versus correlation}: High engagement causes trending, but trending also amplifies engagement through increased exposure. Observational data cannot disentangle this bidirectional causality; controlled experiments would be required. \textbf{Temporal ambiguity}: We do not know when engagement metrics were measured relative to trending status. If metrics were recorded \textit{after} trending began, they partially represent outcomes rather than pure predictors. \textbf{Geographic concentration}: The dataset heavily represents India, limiting cultural and linguistic diversity. Trending patterns likely vary across regions. \textbf{Threshold sensitivity}: Defining trending as rank $\leq 10$ is somewhat arbitrary; different thresholds would yield different models. \textbf{Platform evolution}: YouTube's recommendation algorithm evolves continuously; model performance may degrade over time, necessitating periodic retraining.

\subsection{Comparison with Prior Work}

Unlike engagement-only approaches~\cite{szabo2010predicting, figueiredo2011tube}, our method integrates textual and temporal features through a unified pipeline. Unlike text-only methods requiring large corpora for deep learning~\cite{devlin2018bert}, our TF-IDF approach achieves competitive performance with moderate data volumes. Unlike studies focusing solely on prediction~\cite{borghol2012untold}, we emphasize reproducibility through complete pipeline specification and address practical deployment considerations.

\section{Conclusion and Future Work}

This study demonstrates that supervised machine learning can effectively predict YouTube trending status from metadata and engagement signals. Our Gradient Boosting classifier achieves $F_1 = 0.861$ and AUC = 0.938 on held-out test data, substantially exceeding linear baselines. Engagement rate features normalized by view count emerge as primary predictive signals, suggesting that per-viewer appeal quality matters more than absolute reach magnitude.

Several avenues merit future investigation. \textbf{Hyperparameter optimization}: Systematic tuning via grid search or Bayesian optimization could further improve performance. \textbf{Feature importance analysis}: SHAP values~\cite{lundberg2017unified} would quantify individual feature contributions and enhance interpretability. \textbf{Deep learning}: Transformer-based text encoders~\cite{devlin2018bert} might capture semantic richness beyond TF-IDF, though requiring larger training corpora. \textbf{Temporal modeling}: Rather than binary classification, modeling engagement trajectories over time would provide richer predictions. \textbf{Cross-platform analysis}: Incorporating social media mentions, search trends, and external promotion signals could improve accuracy. \textbf{Causal inference}: Techniques such as propensity score matching or instrumental variables might disentangle causal relationships. \textbf{Production deployment}: Building scalable APIs with model monitoring, drift detection, and automated retraining would enable real-world application.

The systematic methodology employed here—careful preprocessing, rigorous feature engineering, comprehensive model comparison, and honest limitation acknowledgment—transfers to other classification domains: customer churn prediction, fraud detection, medical diagnosis, and beyond. Machine learning models reflect the data they are trained on and the assumptions we encode; continuous critical evaluation remains essential for responsible deployment.

\bibliographystyle{IEEEtran}
\bibliography{references}

\end{document}
